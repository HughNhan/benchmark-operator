---
kind: Job
apiVersion: batch/v1
metadata:
{% if workload_args.serviceip is sameas true %}
  name: 'uperf-client-{{item.spec.clusterIP}}-{{ trunc_uuid }}'
{% else %}
  name: 'uperf-client-{{item.status.podIP}}-{{ trunc_uuid }}'
{% endif %}
  namespace: '{{ operator_namespace }}'
spec:
  template:
    metadata:
      labels:
        app: uperf-bench-client-{{ trunc_uuid }}
        clientfor: {{ item.metadata.labels.app }}
        type: uperf-bench-client-{{ trunc_uuid }}
{% if workload_args.multus.enabled is sameas true %}
      annotations:
        k8s.v1.cni.cncf.io/networks: {{ workload_args.multus.client }}
{% endif %}
    spec:
{% if workload_args.runtime_class is defined %}
      runtimeClassName: "{{ workload_args.runtime_class }}"
{% endif %}
{% if workload_args.hostnetwork is sameas true %}
      hostNetwork: true
      serviceAccountName: benchmark-operator
{% endif %}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - {{ item.metadata.labels.app }}
              topologyKey: kubernetes.io/hostname
      containers:
      - name: benchmark
        image: {{ workload_args.image | default('quay.io/cloud-bulldozer/uperf:latest') }}
        env:
          - name: uuid
            value: "{{ uuid }}"
          - name: test_user
            value: "{{ test_user | default("ripsaw") }}"
          - name: clustername
            value: "{{ clustername }}"
{% if elasticsearch is defined %}
          - name: es
            value: "{{ elasticsearch.server }}"
          - name: es_port
            value: "{{ elasticsearch.port }}"
          - name: es_index
            value: "{{ elasticsearch.index_name | default("ripsaw-uperf") }}"
          - name: parallel
            value: "{{ elasticsearch.parallel | default("false") }}"
{% endif %}
{% if prometheus is defined %}
          - name: prom_es
            value: "{{ prometheus.es_server }}"
          - name: prom_port
            value: "{{ prometheus.es_port }}"
          - name: prom_parallel
            value: "{{ prometheus.es_parallel | default("false") }}"
          - name: prom_token
            value: "{{ prometheus.prom_token | default() }}"
          - name: prom_url
            value: "{{ prometheus.prom_url | default() }}"
{% endif %}
{% if uperf.pin is sameas true %}
          - name: client_node
            client_node: {{ uperf.pin_client }}
          - name: client_node
            server_node: {{ uperf.pin_server }}
{% endif %}
{% if workload_args.client_resources is defined %}
        resources: {{ workload_args.client_resources | to_json }}
{% endif %}
        imagePullPolicy: Always
        command: ["/bin/sh", "-c"]
        args:
{% if workload_args.serviceip is sameas true %}
          - "export serviceip=true;
            export h={{item.spec.clusterIP}};
{% else %}
{% if workload_args.multus.client is defined %}
          - "export multus_client={{workload_args.multus.client}};
            export h={{ (item['metadata']['annotations']['k8s.v1.cni.cncf.io/networks-status'] | from_json)[1]['ips'][0] }};
{% else %}
          - "export h={{item.status.podIP}};
{% endif %}
{% endif %}
{% if workload_args.networkpolicy is defined %}
             export networkpolicy={{workload_args.networkpolicy}};
{% endif %}
             export hostnet={{workload_args.hostnetwork}};
             my_node_idx={{ (item['metadata']['annotations']['node_idx'] | from_json) }};
             my_pod_idx={{ (item['metadata']['annotations']['pod_idx'] | from_json) }};
             export ips=$(hostname -I);
             export num_pairs={{workload_args.pair}};
             node_limit=0;
             pod_limit=0;
             while true; do
               state=$(redis-cli -h {{bo.resources[0].status.podIP}} get start);
               if [[ $state =~ 'true' ]]; then
                   node_limit=$(redis-cli -h {{bo.resources[0].status.podIP}} get node_idx);
                   pod_limit=$(redis-cli -h {{bo.resources[0].status.podIP}} get pod_idx);
                   if [[ $my_node_idx -gt $node_limit || $my_pod_idx -gt $pod_limit ]]; then
                         continue;
                   fi;

                   /bin/echo 'UPERF scale config: num_node=' $node_limit+1' density= {{ pod_idx }}';

{% for test in workload_args.test_types %}
{% for proto in workload_args.protos %}
{% for size in workload_args.sizes %}
{% if size is iterable %}
{% set wsize = size[0] %}
{% set rsize = size[1] %}
{% else %}
{% set wsize = size %}
{% set rsize = size %}
{% endif %}
{% for nthr in workload_args.nthrs %}
                 cat /tmp/uperf-test/uperf-{{test}}-{{proto}}-{{wsize}}-{{rsize}}-{{nthr}};
{% if workload_args.run_id is defined %}
                 run_snafu --tool uperf --run-id {{workload_args.run_id}} -w /tmp/uperf-test/uperf-{{test}}-{{proto}}-{{wsize}}-{{rsize}}-{{nthr}} -s {{workload_args.samples}} --resourcetype {{resource_kind}} -u {{ uuid }} --user {{test_user | default("ripsaw")}};
{% else %}
                 run_snafu --tool uperf -w /tmp/uperf-test/uperf-{{test}}-{{proto}}-{{wsize}}-{{rsize}}-{{nthr}} -s {{workload_args.samples}} --resourcetype {{resource_kind}} -u {{ uuid }} --user {{test_user | default("ripsaw")}};
{% endif %}
{% endfor %}
{% endfor %}
{% endfor %}
{% endfor %}
                   redis-cli -h {{bo.resources[0].status.podIP}} incr num_completion;
                   while true; do
                       state=$(redis-cli -h {{bo.resources[0].status.podIP}} get start);
                       if [[ $state =~ 'restart' ]]; then
                          break;
                       elif [[ $state =~ 'done' ]]; then
                          break;
                       else
                         continue;
                       fi;
                   done;
                   if [[ $state =~ 'restart' ]]; then
                      continue;
                   fi;

               elif [[ $state =~ 'done' ]]; then
                   break;
               else
                 continue;
               fi;
               break;
             done;
             redis-cli -h {{bo.resources[0].status.podIP}} set start false"
        volumeMounts:
          - name: config-volume
            mountPath: "/tmp/uperf-test"
      volumes:
        - name: config-volume
          configMap:
            name: uperf-test-{{ trunc_uuid }}
      restartPolicy: OnFailure
{% if workload_args.max_node is defined %}
{% if workload_args.colocate is sameas true %}
      nodeSelector:
        # client node same as server node
        kubernetes.io/hostname: "{{ worker_node_list[item['metadata']['annotations']['node_idx'] | from_json] }}"
{% else %}
      nodeSelector:
        # skew client node one position left in the woker_node_list
        kubernetes.io/hostname: "{{ worker_node_list[ (1+(item['metadata']['annotations']['node_idx'] | from_json)) % (worker_node_list|length)] }}"
{% endif %}

{% else  %}
{% if workload_args.pin is sameas true %}
      nodeSelector:
        kubernetes.io/hostname: '{{ workload_args.pin_client }}'
{% endif %}
   
{% endif %}


{% include "metadata.yml.j2" %}
